{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45fb3ad-9356-4b80-9d9a-ff5d205b680a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Node' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpomegranate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Rain node has no parents\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m rain \u001b[38;5;241m=\u001b[39m \u001b[43mNode\u001b[49m(DiscreteDistribution({\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheavy\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m      8\u001b[0m }), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#Track maintainace node is conditional on rain\u001b[39;00m\n\u001b[1;32m     11\u001b[0m maintenance \u001b[38;5;241m=\u001b[39m Node(ConditionalProbabilityTable([\n\u001b[1;32m     12\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m0.4\u001b[39m],\n\u001b[1;32m     13\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m0.6\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheavy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m0.9\u001b[39m]],\n\u001b[1;32m     18\u001b[0m     [rain\u001b[38;5;241m.\u001b[39mdistribution]), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaintenance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Node' is not defined"
     ]
    }
   ],
   "source": [
    "from pomegranate import *\n",
    "\n",
    "#Rain node has no parents\n",
    "rain = Node(DiscreteDistribution({\n",
    "    \"none\": 0.7,\n",
    "    \"light\": 0.2,\n",
    "    \"heavy\":0.1\n",
    "}), name=\"rain\")\n",
    "\n",
    "#Track maintainace node is conditional on rain\n",
    "maintenance = Node(ConditionalProbabilityTable([\n",
    "    [\"none\",\"yes\",0.4],\n",
    "    [\"none\",\"no\",0.6],\n",
    "    [\"light\",\"yes\",0.2],\n",
    "    [\"light\",\"no\",0.8],\n",
    "    [\"heavy\",\"yes\",0.1],\n",
    "    [\"heavy\",\"no\",0.9]],\n",
    "    [rain.distribution]), name=\"maintenance\")\n",
    "\n",
    "#Track node is conditional on rain and maintenance\n",
    "train = Node(conditionalProbabilityTable([\n",
    "    [\"none\",\"yes\",\"on time\",0.8],\n",
    "    [\"none\",\"yes\",\"delayed\",0.2],\n",
    "    [\"none\",\"no\",\"on time\",0.9],\n",
    "    [\"none\",\"no\",\"delayed\",0.1],\n",
    "    [\"light\",\"yes\",\"on time\",0.6],\n",
    "    [\"light\",\"yes\",\"delayed\",0.4],\n",
    "    [\"light\",\"no\",\"on time\",0.7],\n",
    "    [\"light\",\"no\",\"delayed\",0.3],\n",
    "    [\"heavy\",\"yes\",\"on time\",0.4],\n",
    "    [\"heavy\",\"yes\",\"delayed\",0.6],\n",
    "    [\"heavy\",\"no\",\"on time\",0.5],\n",
    "    [\"heavy\",\"no\",\"delayed\",0.5],\n",
    "],[rain.distribution,maintenance.distribution]), name=\"train\")\n",
    "\n",
    "#Appointment node is conditional on train\n",
    "appointment = Node(ConditionalProbabilityTable([\n",
    "    [\"on time\", \"attend\", 0.9],\n",
    "    [\"on time\", \"miss\", 0.1],\n",
    "    [\"delayed\", \"attend\",0.6],\n",
    "], [train.distibution]), name=\"appointment\")\n",
    "\n",
    "#create a bayesian network and add states\n",
    "model = BayesianNetwork()\n",
    "model.add_states(rain,maintenance,train,appointment)\n",
    "\n",
    "# Add edges connecting nodes\n",
    "model.add_edge(rain, maintenance)\n",
    "model.add_edge(rain, train)\n",
    "model.add_edge(maintenance, train)\n",
    "\n",
    "model.add_edge(train, appointment)\n",
    "\n",
    "# Finalize model\n",
    "model.bake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ea947-1226-4795-8c57-3175dbad18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pomegranate import *\n",
    "try:\n",
    "    # Define starting probabilities\n",
    "    start = DiscreteDistribution({\n",
    "        \"sun\":0.5,\n",
    "        \"rain\":0.5\n",
    "    })\n",
    "    \n",
    "    # Define transition model\n",
    "    transistions = ConditionalProbabilityTable([\n",
    "        [\"sun\", \"sun\", 0.8],\n",
    "        [\"sun\", \"rain\",0.2],\n",
    "        [\"rain\",\"sun\",0.3],\n",
    "        [\"rain\",\"rain\",0.7]\n",
    "    ], [start])\n",
    "    \n",
    "    # Create Markov chain\n",
    "    model = MarkovChain([start, transistions])\n",
    "    \n",
    "    # sample 50 states from chain\n",
    "    print(model.sample(50))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb945d7d-6ff1-4413-84c0-9638be4fcd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pomegranate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d8a3a-49db-4565-92e7-b2459fd963cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe9d4f-0983-48ff-84bf-222ad3f6df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf66abc-2437-4c21-8300-235bf782f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openAi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62109a2f-c873-469d-bb97-05a7be255b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "# Define your desired output structure\n",
    "class UserInfo(BaseModel):\n",
    "    name:str\n",
    "    age:int\n",
    "\n",
    "#patch the openAi client\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "#Extract structured Data from natural language\n",
    "user_info = client.chat.completions.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        response_model =UserInfo,\n",
    "        message=[{\"role\":\"user\",\"content\":\"Brian Kimanzi is 22 years old.\"}],\n",
    ")\n",
    "print(user_info.name)\n",
    "print(user_info.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50fe63be-a86f-47ea-986f-c6c609ba7a8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '4/29/2020'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[1;32m     14\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m---> 15\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevidence\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m row[:\u001b[38;5;241m4\u001b[39m]],\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     17\u001b[0m         })\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Separate data into training and testing groups\u001b[39;00m\n\u001b[1;32m     20\u001b[0m evidence \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevidence\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '4/29/2020'"
     ]
    }
   ],
   "source": [
    "# A neural network that learns from a data and categorizes if the data is counterfeit or not\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read data in from file\n",
    "with open() as f: #input file that contain the data\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "\n",
    "    data = []\n",
    "    for row in reader:\n",
    "        data.append({\n",
    "            \"evidence\": [float(cell) for cell in row[:4]],\n",
    "            \"label\": 1 if row[4] == \"0\" else 0\n",
    "        })\n",
    "\n",
    "# Separate data into training and testing groups\n",
    "evidence = [row[\"evidence\"] for row in data]\n",
    "labels = [row[\"label\"] for row in data]\n",
    "x_training, x_testing, y_training, y_testing = train_test_split(\n",
    "    evidence ,labels, test_size=0.4\n",
    "    \n",
    ")\n",
    "\n",
    "# Create a neural network\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Add a hidden layer with 8 units, with ReLU activation\n",
    "model.add(tf.keras.layers.Dense(8, input_shape=(4,), activation=\"relu\"))\n",
    "\n",
    "# Add output layer with 1 unit, with sigmoid activation\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Train neural network\n",
    "model.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.fit(x_training, y_training, epochs=20)\n",
    "\n",
    "# Evaluate how well model performs\n",
    "model.evaluate(x_testing, y_testing, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9c0d959-ac93-4bfa-9502-42b2003c091f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Usage: Python filter.py filename",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Usage: Python filter.py filename\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/jupyterlab/4.1.5/libexec/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#filter Image\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "# Ensure correct usage\n",
    "if len(sys.argv) !=2:\n",
    "    sys.exit(\"Usage: Python filter.py filename\")\n",
    "\n",
    "# Open image\n",
    "image = Image.open(sys.arvg[1]).covert(\"RGB\")\n",
    "\n",
    "# Filter mage according to edge detection kernel\n",
    "filtered = image.filter(ImageFilter.Kernel(\n",
    "    size =(3, 3),\n",
    "    kernel=[-1, -1, -1, -1, 8, -1, -1, -1, -1],\n",
    "    scale=1\n",
    "))\n",
    "# show resulting image\n",
    "filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535889c5-577e-4807-abed-d37d6c1cd959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 19ms/step - accuracy: 0.8580 - loss: 0.4536\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 19ms/step - accuracy: 0.9671 - loss: 0.1087\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 24ms/step - accuracy: 0.9746 - loss: 0.0824\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 26ms/step - accuracy: 0.9805 - loss: 0.0638\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 29ms/step - accuracy: 0.9830 - loss: 0.0546\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 21ms/step - accuracy: 0.9843 - loss: 0.0492\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 23ms/step - accuracy: 0.9874 - loss: 0.0395\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 21ms/step - accuracy: 0.9874 - loss: 0.0380\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 22ms/step - accuracy: 0.9901 - loss: 0.0299\n",
      "Epoch 10/10\n",
      "\u001b[1m1844/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9899 - loss: 0.0288"
     ]
    }
   ],
   "source": [
    "# Handwriting Recognision accuracy  \n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "# use MNIST handwriting dataset\n",
    "mnist = tf.keras.datasets.mnist # MNIST a dataset of a numerous handwritten samples of people handwritting \n",
    "\n",
    "# prepare data fro training\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "x_train = x_train.reshape(\n",
    "    x_train.shape[0], x_train.shape[1], x_train.shape[2], 1\n",
    ")\n",
    "x_test = x_test.reshape(\n",
    "    x_test.shape[0], x_test.shape[1], x_test.shape[2], 1\n",
    ")\n",
    "# Create a convolutional neural network\n",
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "    # Convolutional layer, Learn 32 filters using a 3*3 kernel\n",
    "    tf.keras.layers.Conv2D(\n",
    "        32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)\n",
    "    ),\n",
    "\n",
    "    # Max-Pooling layer, using 2*2 pool size\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Flatten units\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    # Add a hidden layer with dropout\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    # Add an output layer with output units for all 10 digits\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\") # softmax will take the output and turn it into a probabilty distribution\n",
    "])\n",
    "\n",
    "# Train neural network\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(x_train, y_train, epochs=10)\n",
    "\n",
    "# Evaluate neural network performance\n",
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "# Save model to file\n",
    "if len(sys.argv) == 2:\n",
    "    filename = sys.argv[1]\n",
    "    model.save(filename)\n",
    "    print(f\"Model saved to {filename}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a318b9b-674d-41bc-90ee-5b0f5de7f912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac76a31-55b6-483f-8ac8-a1435523ee8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
