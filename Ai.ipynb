{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45fb3ad-9356-4b80-9d9a-ff5d205b680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pomegranate import *\n",
    "\n",
    "#Rain node has no parents\n",
    "rain = Node(DiscreteDistribution({\n",
    "    \"none\": 0.7,\n",
    "    \"light\": 0.2,\n",
    "    \"heavy\":0.1\n",
    "}), name=\"rain\")\n",
    "\n",
    "#Track maintainace node is conditional on rain\n",
    "maintenance = Node(ConditionalProbabilityTable([\n",
    "    [\"none\",\"yes\",0.4],\n",
    "    [\"none\",\"no\",0.6],\n",
    "    [\"light\",\"yes\",0.2],\n",
    "    [\"light\",\"no\",0.8],\n",
    "    [\"heavy\",\"yes\",0.1],\n",
    "    [\"heavy\",\"no\",0.9]],\n",
    "    [rain.distribution]), name=\"maintenance\")\n",
    "\n",
    "#Track node is conditional on rain and maintenance\n",
    "train = Node(conditionalProbabilityTable([\n",
    "    [\"none\",\"yes\",\"on time\",0.8],\n",
    "    [\"none\",\"yes\",\"delayed\",0.2],\n",
    "    [\"none\",\"no\",\"on time\",0.9],\n",
    "    [\"none\",\"no\",\"delayed\",0.1],\n",
    "    [\"light\",\"yes\",\"on time\",0.6],\n",
    "    [\"light\",\"yes\",\"delayed\",0.4],\n",
    "    [\"light\",\"no\",\"on time\",0.7],\n",
    "    [\"light\",\"no\",\"delayed\",0.3],\n",
    "    [\"heavy\",\"yes\",\"on time\",0.4],\n",
    "    [\"heavy\",\"yes\",\"delayed\",0.6],\n",
    "    [\"heavy\",\"no\",\"on time\",0.5],\n",
    "    [\"heavy\",\"no\",\"delayed\",0.5],\n",
    "],[rain.distribution,maintenance.distribution]), name=\"train\")\n",
    "\n",
    "#Appointment node is conditional on train\n",
    "appointment = Node(ConditionalProbabilityTable([\n",
    "    [\"on time\", \"attend\", 0.9],\n",
    "    [\"on time\", \"miss\", 0.1],\n",
    "    [\"delayed\", \"attend\",0.6],\n",
    "], [train.distibution]), name=\"appointment\")\n",
    "\n",
    "#create a bayesian network and add states\n",
    "model = BayesianNetwork()\n",
    "model.add_states(rain,maintenance,train,appointment)\n",
    "\n",
    "# Add edges connecting nodes\n",
    "model.add_edge(rain, maintenance)\n",
    "model.add_edge(rain, train)\n",
    "model.add_edge(maintenance, train)\n",
    "\n",
    "model.add_edge(train, appointment)\n",
    "\n",
    "# Finalize model\n",
    "model.bake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ea947-1226-4795-8c57-3175dbad18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pomegranate import *\n",
    "try:\n",
    "    # Define starting probabilities\n",
    "    start = DiscreteDistribution({\n",
    "        \"sun\":0.5,\n",
    "        \"rain\":0.5\n",
    "    })\n",
    "    \n",
    "    # Define transition model\n",
    "    transistions = ConditionalProbabilityTable([\n",
    "        [\"sun\", \"sun\", 0.8],\n",
    "        [\"sun\", \"rain\",0.2],\n",
    "        [\"rain\",\"sun\",0.3],\n",
    "        [\"rain\",\"rain\",0.7]\n",
    "    ], [start])\n",
    "    \n",
    "    # Create Markov chain\n",
    "    model = MarkovChain([start, transistions])\n",
    "    \n",
    "    # sample 50 states from chain\n",
    "    print(model.sample(50))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb945d7d-6ff1-4413-84c0-9638be4fcd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pomegranate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d8a3a-49db-4565-92e7-b2459fd963cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe9d4f-0983-48ff-84bf-222ad3f6df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf66abc-2437-4c21-8300-235bf782f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openAi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62109a2f-c873-469d-bb97-05a7be255b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "# Define your desired output structure\n",
    "class UserInfo(BaseModel):\n",
    "    name:str\n",
    "    age:int\n",
    "\n",
    "#patch the openAi client\n",
    "client = instructor.from_openai(OpenAI())\n",
    "\n",
    "#Extract structured Data from natural language\n",
    "user_info = client.chat.completions.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        response_model =UserInfo,\n",
    "        message=[{\"role\":\"user\",\"content\":\"Brian Kimanzi is 22 years old.\"}],\n",
    ")\n",
    "print(user_info.name)\n",
    "print(user_info.age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe63be-a86f-47ea-986f-c6c609ba7a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neural network that learns from a data and categorizes if the data is counterfeit or not\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read data in from file\n",
    "with open('/Users/briankimanzi/Downloads/banknotes.csv') as f: #input file that contain the data\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "\n",
    "    data = []\n",
    "    for row in reader:\n",
    "        data.append({\n",
    "            \"evidence\": [float(cell) for cell in row[:4]],\n",
    "            \"label\": 1 if row[4] == \"0\" else 0\n",
    "        })\n",
    "\n",
    "# Separate data into training and testing groups\n",
    "evidence = [row[\"evidence\"] for row in data]\n",
    "labels = [row[\"label\"] for row in data]\n",
    "x_training, x_testing, y_training, y_testing = train_test_split(\n",
    "    evidence ,labels, test_size=0.4\n",
    "    \n",
    ")\n",
    "\n",
    "# Create a neural network\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Add a hidden layer with 8 units, with ReLU activation\n",
    "model.add(tf.keras.layers.Dense(8, input_shape=(4,), activation=\"relu\"))\n",
    "\n",
    "# Add output layer with 1 unit, with sigmoid activation\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Train neural network\n",
    "model.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.fit(x_training, y_training, epochs=20)\n",
    "\n",
    "# Evaluate how well model performs\n",
    "model.evaluate(x_testing, y_testing, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0d959-ac93-4bfa-9502-42b2003c091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter Image\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "# Ensure correct usage\n",
    "if len(sys.argv) !=2:\n",
    "    sys.exit(\"Usage: Python filter.py filename\")\n",
    "\n",
    "# Open image\n",
    "image = Image.open(sys.arvg[1]).convert(\"RGB\") \n",
    "\n",
    "# Filter mage according to edge detection kernel\n",
    "filtered = image.filter(ImageFilter.Kernel(\n",
    "    size =(3, 3),\n",
    "    kernel=[-1, -1, -1, -1, 8, -1, -1, -1, -1],\n",
    "    scale=1\n",
    "))\n",
    "# show resulting image\n",
    "filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "535889c5-577e-4807-abed-d37d6c1cd959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 10:49:06.609966: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/Cellar/jupyterlab/4.1.5/libexec/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 23ms/step - accuracy: 0.8643 - loss: 0.4478\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 21ms/step - accuracy: 0.9652 - loss: 0.1129\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 22ms/step - accuracy: 0.9762 - loss: 0.0785\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 31ms/step - accuracy: 0.9808 - loss: 0.0653\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 28ms/step - accuracy: 0.9819 - loss: 0.0577\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 31ms/step - accuracy: 0.9854 - loss: 0.0445\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 34ms/step - accuracy: 0.9871 - loss: 0.0385\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 34ms/step - accuracy: 0.9886 - loss: 0.0349\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 29ms/step - accuracy: 0.9898 - loss: 0.0313\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 24ms/step - accuracy: 0.9898 - loss: 0.0308\n",
      "313/313 - 2s - 6ms/step - accuracy: 0.9883 - loss: 0.0369\n"
     ]
    }
   ],
   "source": [
    "# Handwriting Recognision accuracy  \n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "# use MNIST handwriting dataset\n",
    "mnist = tf.keras.datasets.mnist # MNIST a dataset of a numerous handwritten samples of people handwritting \n",
    "\n",
    "# prepare data fro training\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "x_train = x_train.reshape(\n",
    "    x_train.shape[0], x_train.shape[1], x_train.shape[2], 1\n",
    ")\n",
    "x_test = x_test.reshape(\n",
    "    x_test.shape[0], x_test.shape[1], x_test.shape[2], 1\n",
    ")\n",
    "# Create a convolutional neural network\n",
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "    # Convolutional layer, Learn 32 filters using a 3*3 kernel\n",
    "    tf.keras.layers.Conv2D(\n",
    "        32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)\n",
    "    ),\n",
    "\n",
    "    # Max-Pooling layer, using 2*2 pool size\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Flatten units\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    # Add a hidden layer with dropout\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    # Add an output layer with output units for all 10 digits\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\") # softmax will take the output and turn it into a probabilty distribution\n",
    "])\n",
    "\n",
    "# Train neural network\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(x_train, y_train, epochs=10)\n",
    "\n",
    "# Evaluate neural network performance\n",
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "# Save model to file\n",
    "if len(sys.argv) == 2:\n",
    "    filename = sys.argv[1]\n",
    "    model.save(filename)\n",
    "    print(f\"Model saved to {filename}.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a318b9b-674d-41bc-90ee-5b0f5de7f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "# describe a context-free grammar\n",
    "    s -> NP VP\n",
    "\n",
    "    NP -> D N | N\n",
    "\n",
    "    VP -> V | V NP\n",
    "\n",
    "    D -> \"the\" | \"a\"\n",
    "    N -> \"she\" | \"city\" | \"car\"\n",
    "    V -> \"saw\" | \"walked\"\n",
    "\"\"\")\n",
    "\n",
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "sentence = input(\"sentence: \").split()\n",
    "try:\n",
    "    for tree in parser.parse(sentence):\n",
    "        tree.pretty_print()\n",
    "\n",
    "except ValueError:\n",
    "    print(\"No parse tree possible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ac76a31-55b6-483f-8ac8-a1435523ee8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "sentence:  she saw the dog with the binoculars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     s                                     \n",
      "  ___|___________                           \n",
      " |               VP                        \n",
      " |    ___________|________                  \n",
      " |   |       |            PP               \n",
      " |   |       |        ____|___              \n",
      " |   |       NP      |        NP           \n",
      " |   |    ___|___    |     ___|______       \n",
      " NP  |   |       NP  |    |          NP    \n",
      " |   |   |       |   |    |          |      \n",
      " N   V   D       N   P    D          N     \n",
      " |   |   |       |   |    |          |      \n",
      "she saw the     dog with the     binoculars\n",
      "\n",
      "     s                                     \n",
      "  ___|_______                               \n",
      " |           VP                            \n",
      " |    _______|___                           \n",
      " |   |           NP                        \n",
      " |   |    _______|____                      \n",
      " |   |   |            NP                   \n",
      " |   |   |    ________|___                  \n",
      " |   |   |   |            PP               \n",
      " |   |   |   |    ________|___              \n",
      " |   |   |   |   |            NP           \n",
      " |   |   |   |   |         ___|______       \n",
      " NP  |   |   |   |        |          NP    \n",
      " |   |   |   |   |        |          |      \n",
      " N   V   D   N   P        D          N     \n",
      " |   |   |   |   |        |          |      \n",
      "she saw the dog with     the     binoculars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "# describe a context-free grammar\n",
    "    s -> NP VP\n",
    "\n",
    "    AP -> A | A AP        \n",
    "    NP -> N | D NP | AP NP | N PP       \n",
    "    PP -> P NP          \n",
    "    VP -> V | V NP | V NP PP        \n",
    "\n",
    "    A -> \"big\" | \"blue\" | \"small\" | \"dry\" | \"wide\"\n",
    "    D -> \"the\" | \"a\" | \"an\"\n",
    "    N -> \"she\" | \"city\" | \"car\" | \"street\" | \"dog\" | \"binoculars\"\n",
    "    P -> \"on\" | \"over\" | \"before\" | \"below\" | \"with\"\n",
    "    V -> \"saw\" | \"walked\"\n",
    "\"\"\")\n",
    "\n",
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "sentence = input(\"sentence: \").split()\n",
    "try:\n",
    "    for tree in parser.parse(sentence):\n",
    "        tree.pretty_print()\n",
    "\n",
    "except ValueError:\n",
    "    print(\"No parse tree possible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b6d490-4c49-4f32-845b-d7c4ec6e1512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "import sys\n",
    "\n",
    "# Read text from file\n",
    "if len(sys.argv) != 2:\n",
    "    sys.exit(\"Usage python generator.py sample.txt\")\n",
    "with open(sys.argv[1]) as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Train model\n",
    "text_model = markovify.Text(text)\n",
    "\n",
    "# Generate sentences \n",
    "print()\n",
    "for i in range(5):\n",
    "    print(text_model.make_sentence())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c7a1e-f599-4d5f-a2f0-311e0c08644e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba94325-38ca-4152-a0bf-ccf63d949969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
